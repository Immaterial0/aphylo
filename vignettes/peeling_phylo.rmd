---
title: "Peeling Algorithm in Phylogenic Trees"
date: "`r paste('This version', Sys.Date())`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Peeling Algorithm in Phylogenic Trees}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

\newcommand{\isone}[1]{{\boldsymbol{1}\left\{ #1 \right\}}}
\renewcommand{\Pr}[1]{{\mbox{Pr}\left(#1\right) }}
\renewcommand{\f}[1]{{f\left(#1\right) }}
\newcommand{\Prcond}[2]{{\mbox{Pr}\left(#1\;|\;#2\right) }}
\newcommand{\fcond}[2]{{f\left(#1\;|\;#2\right) }}

# Definitions

## Anotated Phylogenetic Trees

A phylogenetic tree $\tau\equiv (N,E)$ is a tuple of nodes $N$, and edges $E\equiv \{(n, m) \in N\times N: n\mapsto m\}$ defined by the binary operator $\mapsto$ _parent of_. Furthermore, we define $O(n)\equiv\{m\in N: (n, m) \in E\}$ as the set of offspring of $n$, and $r(m) \equiv\{n \in N: (n, m) \in E\}$ the parent nodes of $m$. For now on we assume that $|r(m)| \leq 1,\forall n\in N$. 

Given $P$ gene functions, let $A \equiv \{a_n \in \{0,1\}^P: n\in N\}$ denote a set of genetic annotations--which we will also refer as states--of the genes $N$. We define an Annotated Phylogenetic Tree as the tuple $D \equiv(\tau, A)$. Furthermore, we say that this structure is a Partially Ordered Annotated Phylogenetic Tree if it's nodes labels form a partial order, this is $E\equiv \{(n, m) \in N\times N: n < m\}$.

While $A$ exists, we only observe an imperfect approximation of it, experimental data $\tilde A = \{\tilde a_l\}_{l\in N}$, which only holds information for some of the leaf nodes. Therefore, for any given leaf node $l$, the elements of $\{\tilde a_{lp}\}_{p=1}^P$ can take the following values:

$$
\tilde a_{lp} = \left\{
\begin{array}{ll}
1 & \mbox{if the function }p\mbox{ is cosidered as active}\\
0 & \mbox{if the function }p\mbox{ is cosidered as non-active}\\
9 & \mbox{if we don't have information }
\end{array}\right.
$$

This way, assuming that we observe the true phylogenetic tree, we denote the tuple $\tilde D\equiv(\tau, \tilde A)$ to be a Experimentally Annotated Phylogenetic Tree. Ultimately, we are interested in predicting functional annotations for the leaf nodes that have not been annotated yet.

## Likelihood of an Anotated Phylogenetic Tree

Given $\tilde D$, the probability that the true state is of the $l$-leaf is $a_l = \{a_{lp}\}_{p=1}^P$ is:

$$ 
\label{eq:1a}\tag{1a}
\Prcond{a_l}{\tilde D, \psi} = \prod_{p = 1}^P\left\{\left[\psi_0^{\tilde a_{lp}}(1-\psi_0)^{1- \tilde a_{lp}}\right]^{1- a_{lp}} \left[\psi_1^{1- \tilde a_{lp}}(1-\psi_1)^{\tilde a_{lp}}\right]^{a_{lp}} \right\}^{\left[1 - \isone{\tilde a_{lp} !=9}\right]}
$$ 

Where $\psi\equiv\{\psi_0, \psi_1\}$ is a vector of missclassification probabilities, formally:

$$
\psi_0 = \Prcond{\tilde a_{lp}=1}{a_{lp} = 0},\quad
\psi_1 = \Prcond{\tilde a_{lp}=0}{a_{lp} = 1}
$$

Computationally, observe that the largest parenthesis needs to be calculated only once and then retrieved depending on the values of $(a_{lp},\tilde a_{lp})$. Let $S$ be a square matrix defined as follows

$$
S = \left[\begin{array}{cc}
s_{00} & s_{01} \\
s_{10} & s_{11}
\end{array}\right]
=\left[\begin{array}{cc}
1-\psi_0 & \psi_0 \\
\psi_1 & 1 - \psi_1
\end{array}\right]
$$

Then, (1a) can be reexpress using $S$:

$$ 
\label{eq:1b}\tag{1b}
\Prcond{a_l}{\tilde D, \psi} = \prod_{p = 1}^P {s_{a_{lp} \tilde a_{lp}}}^{\left[1 - \isone{\tilde a_{lp} !=9}\right]}
$$

For any internal node $n \in N$, the probability of observing a given state is defined in terms of its offspring $O(n)$, and parameters $(\psi, \mu)$, where $\mu \equiv \{\mu_0,\mu_1\}$ is a vector of probabilities of gain and loss of a function, formally

$$
\mu_0 = \Prcond{a_{lp} = 1}{a_{r(l)p} = 0},\quad  \mu_0 = \Prcond{a_{lp} = 0}{a_{r(l)p} = 1}
$$

This way, the probability of observing state $a_n = \{a_{np}\}_{p=1}^P$ is

 
$$
\tag{2}
\Prcond{a_n}{\tilde D, \psi,\mu} = \prod_{m \in O_n} \sum_{a_m \in \{0,1\}^P} \Prcond{a_m}{\tilde D, \psi,\mu}
\prod_{p = 1}^P \left\{\left[\mu_0^{a_{mp}}(1-\mu_0)^{1 - a_{mp}}\right]^{1 - a_{np}}
  \left[\mu_1^{1 - a_{mp}}(1-\mu_1)^{a_{mp}}\right]^{a_{np}}\right\}
$$

Observe that if $m\in O(n)$ is a leaf node, then $\Prcond{a_m}{\tilde D, \psi,\mu} = \Prcond{a_m}{\tilde D, \psi}$ which was defined in (1b).

Let $M\equiv\{m_{ij}\}_{i,j \in \{0,1\}}$ to be an array of size $2\times 2$ holding the Gain/Loss probabilities, formally 

$$
M = \left[\begin{array}{cc}
m_{00} & m_{01} \\
m_{10} & m_{11}
\end{array}\right]
= \left[\begin{array}{cc}
1-\mu_0 & \mu_0 \\
\mu_1 & 1 - \mu_1
\end{array}\right]
$$

This way, (2a) can be reexpress as follows

$$
\tag{2b}
\Prcond{a_n}{\tilde D, \psi,\mu} = \prod_{m \in O_n} \sum_{a_m \in \{0,1\}^P} \Prcond{a_m}{\tilde D, \psi,\mu}
\prod_p m_{a_{np}a_{mp}}
$$

Finally, let $\pi\equiv\{\pi_p\}_{p=1}^{P}$ to be the root node state probabilities for each function, then, the likelihood of observing a particular set of annotations conditional on the tree structure and model parameters can be computed using the root node states probabilities:

$$
\tag{3}
\Prcond{\tilde A}{\tau,\psi, \mu, \pi} = \sum_{a_0 \in \{0,1\}^P} \Prcond{a_0}{\pi} \Prcond{a_0}{\tilde D,\psi,\mu}
$$

where $\Prcond{a_0}{\pi} = \prod_{p=1}^P \pi_p^{a_{0p}}\left(1 - \pi_p\right)^{1 - a_{0p}}$.

# Estimation

## Maximum Likelihood

Let $\theta = (\psi, \mu, \pi)$ denote the vector of model parameters. Furthermore, $\theta \in \Psi\times \mathcal{M} \times \Pi$, then, the maximun likelihood estimator $\hat \theta$ is

$$
(\hat \psi, \hat \mu, \hat \pi) = \hat\theta = \arg \max_{\theta \in \Theta} \log \Prcond{\tilde A}{\tau, \theta}
$$


## Markov Chain Monte Carlo with Reflective Boundaries Kernel

# Assesment of Model Predictions

Let $A_H$ and $\hat A_H$ be the observed and predicted annotations for the set of nodes $H$. We write $W_H$ to refer to the undirected shortest path length matrix in which each one of its entries $w_{Hnm}$ holds shortest path length between nodes $n, m$ $\in H$. Then, we assess the prediction quality by the following statistic

$$
\left\{\left\| a_h - \hat a_h \right\|\right\}_{h \in H}^\mathbf{t}
\left(W_H + I_{|H|}\right)^{-1}
\left\{\left\| a_h - \hat a_h \right\|\right\}_{h \in H}
$$

Where $I_{|H|}$ is the identity matrix of size $|H|^2$, and $\|\cdot\|$ is the $\ell^2$-norm.

It can be shown that the previous expression can be written as

$$
\delta = \sum_{i,j}\left[\sum_{p,r} (a_{ip} - \hat a_{ir})^2(a_{jr} - \hat a_{jr})^2\right]^{1/2}w_{ij}^{-1}
$$

So, $\mbox{E}\left(\delta\right)$ reduces to computing the following

$$
\sum_{i,j} w_{ij}^{-1}\mbox{E}\left[\sum_{p,r} (a_{ip} - \hat a_{ir})^2(a_{jr} - \hat a_{jr})^2\right]^{1/2}
$$

# Data Imputation

The ultimate goal of this model is to be able to predict, or rather, impute functional states to leaf nodes for which we do not have information. Using both, the information that we have about the annotated tree and the parameter estimates of gain and loss probabilities, we can calculate what is the probability of observing either a 0 or a 1. Formally, with $S\equiv (N, E)$, the probability of leaf node $n$ having state $a_l = \{a_{lp}\}_{p=1}^P$, conditional on the tree structure $\tilde D S^*$ and the observed set of annotations $A^*$ is:

$$
\tag{1}
\Prcond{a_{np} = 1}{\tau, \tilde A} = \frac{\Prcond{\tilde A}{\tau, a_{np} = 1} \Prcond{a_{np}=1}{\tau}}{
\Prcond{\tilde A}{\tau}
}
$$

For ease of notation, we will drop the $\tau$, as all probabilities throughout this section are conditional on the tree structure. Now, we know that

$$
\tag{2}\label{eq:2}
\Pr{\tilde A} = \Prcond{\tilde A}{a_{np} = 1} \Pr{a_{np} = 1} + \Prcond{\tilde A}{a_{np} = 0} (1 - \Pr{a_{np} = 1})
$$

Hence, pluging in (2) into (1), and multiplying both numerator and denominator by $1/\Pr{a_{np} = 1}$, (1) can be rewritten as

$$
\tag{1'}
\Prcond{a_{np} = 1}{\tilde A} = \frac{\Prcond{\tilde A}{a_{np} = 1}}{
\Prcond{\tilde A}{a_{np} = 1} + \Prcond{\tilde A}{a_{np} = 0} \frac{\left(1 - \Pr{a_{np} = 1}\right)}{\Pr{a_{np} = 1}}
}
$$
Where $\Prcond{\tilde A}{a_{np} = 1}$ is the likelihood of the data given that we set the ith node to be 1, which we already know how to compute. This way, we are only missing $\Pr{a_{np=1}}$, which we can compute as follows:

$$
\begin{align}
\Pr{a_{np} = 1} & = \pi_p \Prcond{a_{np} = 1}{a_{0p} = 1} + (1 - \pi_p) \Prcond{a_{np} = 1}{a_{0p} = 0}
\end{align}
$$

Where, given that $\gamma_{01}$ is the shorest path length between node 0 and node $i$, we have:

$$
\left[\begin{array}{cc}
\Prcond{a_i = 0}{a_0 = 0} & \Prcond{a_i = 1}{a_0 = 0} \\
\Prcond{a_i = 0}{a_0 = 1} & \Prcond{a_i = 1}{a_0 = 1}
\end{array}
\right]
=
\left[\begin{array}{cc}
1 & 0 \\
0 & 1
\end{array} \right] \times
\left[\begin{array}{cc}
1 - \hat\mu_0 &  \hat\mu_0 \\
\hat\mu_1 &  1 - \hat\mu_1
\end{array}
\right]^{\gamma_{0i}}
$$


# Data

The model uses two different datasets: (1) experimental data, which holds functions indicators, and (2) phylogenetic tree data, which contains the parent/offspring relations.

Given how the algorithm for computing the likelihood has been programmed, it is necessary that the experimental dataset must have as many rows as nodes (parents and offspring) there are. To fulfill such requirement, the package `phylogenetic`, throught the function `prepare_data`, ``completes'' the experimental dataset as follows:

1.  List all nodes in the tree that are not in the experimental dataset.

2.  Once identified, if any, add those nodes to the experimental dataset. The
    function indicator columns will have value '9' (unknown). Furthermore, the
    added nodes are tagged so that the user can identify them later.
    
3.  The new experimental dataset is sorted increasingly according to the node
    id number. The idea is that the root, node 0, should appear in the
    first row.
    
Once the experimental data has been processed, 
    
4.  We identify

# Maximum a Posteriori (MAP) Estimation

Using bayes we have

$$
\fcond{\theta}{D} = \frac{\fcond{X}{\theta}\f{\theta}}{f{X}}
$$

Then, assumming iid, to estimate $\hat \theta$ we can solve the following problem

$$
\label{eq:maptheta}
\begin{align}
\hat \theta & = \arg\max_{\theta \in \Theta} \frac{\fcond{X}{\theta}\f{\theta}}{\f{X}} \\
 & \mbox{Since the denominator is constant, this is equivalent to} \\
 & = \arg\max_{\theta \in \Theta} \fcond{X}{\theta}\f{\theta} \tag{MAP estimate} 
\end{align}
$$

Observe that under a uniform prior, this is equivalent to the MLE estimate. Now, the variance (what about the covariance) estimator is as follows

$$
\label{eq:mapvar}
\mbox{Var}\left(\hat\theta\right) = \int_{\theta \in \Theta} \left(\hat\theta - \theta\right)^2
\frac{\fcond{X}{\theta}\f{\theta}}{\f{X}} d\theta \tag{MAP variance}
$$

Where the denominator, probability of evidence, can be computed as

$$
\label{eq:d}
\f{X} = \int_{\theta \in \Theta} \fcond{X}{\theta}\f{\theta}d\theta 
$$

Given $\hat \theta$, the variance can be computed using numerical integration.
