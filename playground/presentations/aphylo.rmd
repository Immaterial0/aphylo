---
title: "Project 2: Augmenting functional information about human genes using probabilistic phylogenetic modeling"
short_title: 
author: "George G. Vega Yon \\linebreak[4] vegayon@usc.edu"
date: "October 5, 2017"
institute:
  - Department of Preventive Medicine
  - University of Southern California
short_institute: USC
short_author: Vega Yon
output: 
  uscimage::beamer_USCImage:
    includes:
      in_header: aphylo.sty
    toc: false
    highlight: zenburn
    keep_tex: true
section-titles: false
fontsize: 9pt
handout: true
page-number: true
---


```{r setup, include=FALSE}
knitr::knit_hooks$set(smallsize = function(before, options, envir) {
    if (before) {
        "\\footnotesize\n\n"
    } else {
        "\n\\normalsize\n\n"
    }
})

options(digits = 4)

knitr::opts_chunk$set(echo = FALSE, smallsize=TRUE,
                      out.width = ".8\\linewidth",
                      fig.width = 7, fig.height = 5, fig.align = 'center'
                      )

library(aphylo)
```

## Agenda

\tableofcontents{}

# On Genes and Trees

## Agenda

\tableofcontents[currentsection]

## Overview  

-   A __GO annotation__ is an association between a gene and a GO (Gene Ontology) term describing its function, e.g: A gene can be annotated with the GO term `GO:0016049`, which denotes _cellular growth_.\pause

-   __Phylogenetic Tree__ represents "inferred evolutionary relationships among various biological species or other entities" (wiki), in this context, our entities are genes.\pause

-   __PANTHER Classification System__ (PantherDB), part of the Gene Ontology Consortium, consists on a database of $\sim$ 15,000 phylogenetic trees (gene families), and can be linked to the GO terms.

## Overview (cont.)

-   Manual Curation of GO terms is good but infeasible: \pause Out of all the genes present in PantherDB, only ~9% has been annotated (17 years of work)\pause

-   Today, we present a model that uses both: (1) existing gene functional annotations, and (2) phylogenetic trees to infer annotations on un-annotated genes in a _probabilistic way_ (so it is not a 0/1 prediction).\pause

-   This predicted functional information will serve as prior covariates in Projects 1 and 3. 

<!-- -   This project is the first to make predictions at large scale in a probabilistic manner. -->

# Model

## Agenda {.t}

\tableofcontents[currentsection]


## Some definitions {.t label=definitions}

\begincols

\begincol{.38\linewidth}

\includetikz{simple_tree_names.tex}{.5}

\endcol

\begincol{.58\linewidth}

\footnotesize

| Symbol | Description |
|:--|:--|
$\aphyloObs$ | Observed Annotated Tree |
$\phylo$ | Partially ordered phylogenetic tree (PO tree) |
$O(n)$ | Offspring of node $n$ |
$\aphyloObs_n$ | $n$-induced Annotated Sub-tree |
$\AnnObs$ | Experimental annotation |

Where 

$$
\annObs_{lp} = \left\{
\begin{array}{ll}
1 & \mbox{if the function }p\mbox{ is believed to be present}\\
0 & \mbox{if the function }p\mbox{ is believed to be absent}\\
9 & \mbox{if we don't have information for this node }
\end{array}\right.
$$

\normalsize

\hyperlink{formaldef}{\beamerbutton{Formal definitions}}

\endcol

\endcols

## A probabilistic model of function propagation

1.  For any given node, we can write down the probability of observing a \emph{functional state} as a function of some model parameters and its offspring. \pause
    
2.  This version of our model has five parameters (probabilities): \pause
    
    a.  Root node had a function: $\pi$, 
    b.  Gain of function: $\mu_0$,
    c.  Loss of function: $\mu_1$.
    d.  Misclassification of:
        -   A missing function as present, $\psi_0$, and
        -   A present function as missing, $\psi_1$ \pause

    All five parameters are assumed to be equal across functions, this is, $\pi, \mu_0, \mu_1, \psi_0$, and $\psi_1$ are assumed to be independent of the functions that are analyzed.\pause
    
3.  In this presentation, we will focus on the case that $P = 1$.


# Peeling algorithm

## Agenda {.t}

\tableofcontents[currentsection]s


## Peeling phylogenies {.t label=peelingalgorithm}

Given an Experimentally Annotated (PO) Phylogenetic Tree, the likelihood computation on a single function is as follows. \pause

\def\probmat{\mbox{Pr}{}}

1.  Create an matrix $\probmat$ of size $2 \times |N|$, \pause

2.  For node $n \in \{|N|, |N| - 1, \dots, 1, 0\}$ (the peeling sequence) do: \pause
    
    a.  For $\ann_n\in \{0,1\}$ do: 
        
        a.  Set \color{teal} $\probmat_{n, \ann_n} = \left\{\begin{array}{ll}
        \Prcond{\Ann_n = \ann_n}{\AnnObs_n = \AnnObs_n} & \mbox{If }$n$\mbox{ is a leaf} \\
        \Prcond{\Ann_n = \ann_n}{\aphyloObs_n} & \mbox{otherwise}
        \end{array} 
        \right.$ \color{black}
        
        b.  Next $\ann_n$
    
    b.  Next $n$ \pause
    
3.  At this point the matrix $\probmat$ should be completely filled, so following \eqref{eq:l}, we can compute
    
    $$
    \likelihood{\psi, \mu, \pi}{\aphyloObs} = \sum_{\ann_0\in\{0,1\}}\Prcond{\Ann_0=\ann_0}{\pi}\probmat_{0,\ann_0}
    $$
    
    \pause

Let's see an example! \hyperlink{leafnodesprob}{\beamerbutton{details}}

## Peeling algorithm {.t}

\begincols

\begincol{.28\textwidth}

\includetikz{simple_tree.tex}{.6}

\endcol

\begincol{.68\textwidth}

*   Let's calculate the likelihood of observing this tree with the following parameters:

```{r example-parameters}
psi0 <- .1
psi1 <- .05
mu0  <- .04
mu1  <- .01
Pi   <- .5
```


$$
\begin{aligned}
\psi_0 &= `r psi0` \\
\psi_1 &= `r psi1` \\
\mu_0  &= `r mu0`  \\
\mu_1  &= `r mu1`  \\
\pi    &= `r Pi`  \\
\end{aligned}
$$

\endcol

\endcols

```{r defining-and-printing-tree}

dat <- matrix(
  as.integer(c(0,1,0,3,1,2,3,4,3,5)), ncol=2, byrow=TRUE)
dat <- aphylo:::new_po_tree(
  dat,
  labels      = as.character(0:5),
  edge.length = c(2, 5, 3, 6, 4)
  )

ann <- cbind(
  Function  = c(NA, NA, 0, NA, 1, 0)
  )
dat <- aphylo:::as_aphylo(ann, dat)

LL <- LogLike(
  dat$annotations,
  attr(dat$edges, "offspring"),
  c(psi0, psi1), c(mu0, mu1), Pi, verb_ans = TRUE)

dimnames(LL$Pr) <- list(0:5, paste("State", 0:1))
```

<!-- ## Peeling algorithm (cont. 1) {.t} -->

<!-- \begincols -->

<!-- \begincol{.28\textwidth} -->

<!-- \includetikz{simple_tree.tex}{.6} -->

<!-- \endcol -->

<!-- \begincol{.68\textwidth} -->



<!-- \pause -->

<!-- *   Essentially we need to get from here: An empty Probability Matrix  -->

<!--     ```{r prob-mat-0} -->
<!--     # Printing the colored version of the matrix -->
<!--     Pr <- LL$Pr -->
<!--     Pr[] <- rep(" ", prod(dim(Pr))) -->
<!--     knitr::kable(Pr) -->
<!--     ``` -->

<!--     \pause -->

<!-- *   To here: Filled probabilty Matrix -->

<!--     ```{r prob-mat-1} -->
<!--     # Printing the colored version of the matrix -->
<!--     Pr <- LL$Pr -->
<!--     knitr::kable(Pr) -->
<!--     ``` -->


<!-- \endcol -->

<!-- \endcols -->


## Peeling algorithm (cont. 1) {.t}

\footnotesize

$$
\psi_0 = `r psi0` \qquad \psi_1 = `r psi1` \qquad \mu_0 = `r mu0` \qquad \mu_1 = `r mu1` \qquad \pi = `r Pi`
$$

\normalsize

\begincols

\begincol{.28\textwidth}

\mode<beamer>{
  \only<1>{\includetikz{simple_tree.tex}{.6}}
  \only<2-3>{\includetikz{simple_tree_leaf2.tex}{.6}}
  \only<4-5>{\includetikz{simple_tree_leaf4.tex}{.6}}
  \only<6-7>{\includetikz{simple_tree_leaf5.tex}{.6}}
}

\mode<handout>{
  \includetikz{simple_tree.tex}{.6}
}

\endcol

\begincol{.68\textwidth}

```{r prob-mat-2, results='asis'}
# Printing the colored version of the matrix
Pr <- LL$Pr
Pr[] <- sprintf("%.4f",Pr[])
Pr[c(1,2,4),] <- ""

Pr[3,1] <- paste0("\\onslide<2->{\\color{blue}", Pr[3,1], "}")
Pr[3,2] <- paste0("\\onslide<3->{\\color{red}", Pr[3,2], "}")
Pr[5,1] <- paste0("\\onslide<4->{\\color{orange}", Pr[5,1], "}")
Pr[5,2] <- paste0("\\onslide<5->{\\color{olive}", Pr[5,2], "}")

Pr[6,1] <- paste0("\\onslide<6->{\\color{brown}", Pr[6,1], "}")
Pr[6,2] <- paste0("\\onslide<7->{\\color{gray}", Pr[6,2], "}")

print(xtable::xtable(Pr), sanitize.text.function=function(x) x, comment=FALSE)
```

\footnotesize

$$
\begin{aligned}
\onslide<2->{\Prcond{Z_2=0}{X_2=0} & = 1 - \psi_0 & = \color{blue}{`r 1-psi0`}} \\
\onslide<3->{\Prcond{Z_2=1}{X_2=0} & = \psi_1 &= \color{red}{`r psi1`}} \\\\
\onslide<4->{\Prcond{Z_4=0}{X_4=1} & = \psi_0 & = \color{orange}{`r psi0`}} \\
\onslide<5->{\Prcond{Z_4=1}{X_4=1} & = 1 - \psi_1 &= \color{olive}{`r 1-psi1`}} \\ \\
\onslide<6->{\Prcond{Z_5=0}{X_5=0} & = 1 - \psi_0 & = \color{brown}{`r 1-psi0`}} \\
\onslide<7->{\Prcond{Z_5=1}{X_5=0} & = \psi_1 &= \color{gray}{`r psi1`}}
\end{aligned}
$$

\normalsize

\endcol

\endcols

## Peeling algorithm (cont. 2) {.t}

\footnotesize

$$
\psi_0 = `r psi0` \qquad \psi_1 = `r psi1` \qquad \mu_0 = `r mu0` \qquad \mu_1 = `r mu1` \qquad \pi = `r Pi`
$$

\normalsize


\begincols

\begincol{.28\textwidth}

\mode<beamer>{
  \only<1>{\includetikz{simple_tree.tex}{.6}}
  \only<2-5>{\includetikz{simple_tree_node1.tex}{.6}}
}

\mode<handout>{
  \includetikz{simple_tree.tex}{.6}
}

\endcol

\begincol{.68\textwidth}

```{r prob-mat-3, results='asis'}
# Printing the colored version of the matrix
Pr <- LL$Pr
Pr[] <- sprintf("%.4f",Pr[])
Pr[c(1,4),] <- ""

Pr[3,1] <- paste0("{\\color{blue}", Pr[3,1], "}")
Pr[3,2] <- paste0("{\\color{red}", Pr[3,2], "}")
Pr[2, 1] <- paste0("\\onslide<3->{\\color{violet}{", Pr[2, 1],"}}")
Pr[2, 2] <- paste0("\\onslide<5->{\\color{purple}{", Pr[2, 2],"}}")

print(xtable::xtable(Pr), sanitize.text.function=function(x) x, comment=FALSE)
```

\footnotesize


$$
\begin{aligned}
\onslide<2->{\Prcond{\Ann_1=0}{\aphyloObs_1} & = \Prcond{\Ann_2 = 0}{\AnnObs_2=0}(1 - \mu_0) +\\
  &\qquad \Prcond{\Ann_2 = 1}{\AnnObs_2=0}\mu_0} \\
\onslide<3->{& = {\color{blue} `r Pr[3, 1]`}\times `r 1-mu0` + {\color{red} `r Pr[3, 2]`}\times `r mu0` = \color{violet}{`r LL$Pr[3, 1]* (1-mu0) +  LL$Pr[3, 2]*mu0`}}\\\\
%
\onslide<4->{\Prcond{\Ann_1=1}{\aphyloObs_1} & = \Prcond{\Ann_2 = 0}{\AnnObs_2=0}\mu_1 +\\
  &\qquad \Prcond{\Ann_2 = 1}{\AnnObs_2=0}(1-\mu_1)} \\
\onslide<5->{& = `r Pr[3, 1]`\times `r mu1` + `r Pr[3, 2]`\times `r 1-mu1` = \color{purple}{`r LL$Pr[3, 1]* mu1 +  LL$Pr[3, 2]*(1-mu1)`}}
\end{aligned}
$$


\normalsize

\endcol

\endcols


## Peeling algorithm (cont. 3) {.t}

\footnotesize

$$
\psi_0 = `r psi0` \qquad \psi_1 = `r psi1` \qquad \mu_0 = `r mu0` \qquad \mu_1 = `r mu1` \qquad \pi = `r Pi`
$$

\normalsize


\begincols

\begincol{.28\textwidth}

\mode<beamer>{
  \only<1>{\includetikz{simple_tree.tex}{.6}}
  \only<2-6>{\includetikz{simple_tree_node3.tex}{.6}}
  \only<7>{\includetikz{simple_tree_node0.tex}{.6}}
  \only<8->{\includetikz{simple_tree_likelihood.tex}{.6}}
}

\mode<handout>{
  \includetikz{simple_tree.tex}{.6}
}

\endcol

\begincol{.68\textwidth}

```{r prob-mat-4, results='asis'}
# Printing the colored version of the matrix
Pr <- LL$Pr
Pr[] <- sprintf("%.4f",Pr[])

Pr[5,1] <- paste0("{\\color{orange}", Pr[5,1], "}")
Pr[5,2] <- paste0("{\\color{olive}", Pr[5,2], "}")

Pr[6,1] <- paste0("{\\color{brown}", Pr[6,1], "}")
Pr[6,2] <- paste0("{\\color{gray}", Pr[6,2], "}")

Pr[4, 1] <- paste0("\\onslide<5->{\\color{red}{", Pr[4, 1],"}}")
Pr[4, 2] <- paste0("\\onslide<6->{", Pr[4, 2],"}")
Pr[1, 1] <- paste0("\\onslide<7->{\\color{cyan}{", Pr[1, 1],"}}")
Pr[1, 2] <- paste0("\\onslide<7->{\\color{blue}{", Pr[1, 2],"}}")

print(xtable::xtable(Pr), sanitize.text.function=function(x) x, comment=FALSE)
```

\tiny

$$
\begin{aligned}
\onslide<2->{\Prcond{\Ann_3 = 0}{\aphyloObs_3} & = %
  \prod_{m \in \{4,5\}} \sum_{\ann_m \in \{0,1\}} \Prcond{\Ann_m = \ann_m}{\aphyloObs_m} \Prcond{\Ann_{m} = \ann_{m}}{\Ann_3 = 0}} \\
\onslide<3->{& = \left(%
    {\color{orange} `r LL$Pr[5,1]`} (1 - \mu_0) + {\color{olive} `r LL$Pr[5,2]` }\times \mu_0
  \right)\times\left(%
    {\color{brown} `r LL$Pr[6,1]`} (1 - \mu_0) + {\color{gray} `r LL$Pr[6,2]` }\times \mu_0
  \right) \\}
\onslide<4->{& = \left(%
    {\color{orange} `r LL$Pr[5,1]`} (1 - `r mu0`) + {\color{olive} `r LL$Pr[5,2]`}\times `r mu0`
  \right)\times\left(%
    {\color{brown} `r LL$Pr[6,1]`} (1 - `r mu0`) + {\color{gray} `r LL$Pr[6,2]` }\times `r mu0`
  \right) \\}
\onslide<5->{& = \color{red}{`r (LL$Pr[5,1]*(1 - mu0) + LL$Pr[5,2]*mu0) * (LL$Pr[6,1]*(1 - mu0) + LL$Pr[6,2]*mu0)`} \\} 
\onslide<8->{ & \mbox{Finally, the likelihood of this tree is:} \\
\likelihood{\psi, \mu, \Pi}{\aphyloObs} & = (1-\pi)\Prcond{\Ann_0=0}{\aphyloObs_0} + \pi\Prcond{\Ann_0=1}{\aphyloObs_0}} \\
\onslide<9->{& = (1 - `r Pi`)\times {\color{cyan} `r LL$Pr[1,1]` } + `r Pi`\times {\color{blue} `r LL$Pr[1,2]` } = \mbox{\normalsize `r exp(LL$ll)`}}
\end{aligned}
$$


\normalsize



\endcol

\endcols


# The `aphylo` R package

## Agenda {.t}

\tableofcontents[currentsection]

## `aphylo` in a nutshell

*   Provides a representation of _annotated_ partially ordered trees. \pause

*   Interacts with the `ape` package (most used Phylogenetics R package with ~25K downloads/month) \pause
    
*   Implements the loglikelihood calculation of our model (with C++ under-the-hood).


## Examples: Simulating Trees {.t}

\begincols

\begincol{.48\textwidth}

```{r example_sim_tree, echo=TRUE}
set.seed(80)
tree <- sim_tree(5)
tree
```

\endcol

\begincol{.48\textwidth}

```{r example_sim_ann_tree, echo=TRUE}
atree <- sim_annotated_tree(
  tree = tree, P = 2,
  psi  = c(.05, .05),
  mu   = c(.2, .1),
  Pi   = .01
  )

atree
```

\endcol

\endcols


## Examples: Visualizing annotated data {.c}

\begincols

\begincol{.49\textwidth}

```{r annotated-viz, echo=TRUE, message=FALSE, warning=FALSE, out.width="1\\linewidth", fig.cap="Visualization of annotations and tree structure.", cache=TRUE}
plot(atree)
```

\endcol

\begincol{.49\textwidth}

```{r likelihood-viz, echo=TRUE, message=FALSE, warning=FALSE, out.width="1\\linewidth", fig.cap="LogLikelihood surface of the simulated data"}
plot_LogLike(atree)
```

\endcol

\endcols

## Example: Tree pruning {.t}

*   The peeling algorithm requires visiting all nodes in a tree.\pause
    
*   The fact is, we don't need to go through branches with no annotations, as these are uninformative. \pause So we can prune them, e.g.:\pause

```{r tree-pruning1}
set.seed(1)
x <- sim_tree(25)
```


```{r tree-pruning2, fig.cap=paste("Pruning trees. In the original none of the leaf nodes under 3 and 9 have annotations. After pruning those branches, we go from having", length(attr(x, "labels"))," nodes, to have", length(attr(prune(x,c(3,9)), "labels"))), out.width=".55\\linewidth"}
# What am I pruning
x_pruned <- prune(x, c(3, 19))

ids        <- attr(x, "labels")
ids_pruned <- attr(x_pruned, "labels")

z   <- `attributes<-`(x, list(dim=dim(x)))
z[] <- ids[z[]+1]

z_pruned   <- `attributes<-`(x_pruned, list(dim=dim(x_pruned)))
z_pruned[] <- ids_pruned[z_pruned[]+1]

cols <- apply(z, 1, function(w) any(w[1] == z_pruned[,1] & w[2] == z_pruned[,2]))
cols <- c("tomato", "steelblue")[cols + 1L]

oldpar <- par(no.readonly=TRUE)
par(mfrow=c(1,2), cex=.9, xpd=NA, mar=c(1,0,2,0))
plot(x, main="Full tree\n x", edge.color = cols, edge.width=2)
plot(x_pruned, main="Pruned tree\n prune(x, c(3, 19))", edge.color = "steelblue", edge.width=2)
par(oldpar)

```

# The `amcmc` R package

## Agenda {.t}

\tableofcontents[currentsection]

## Yet another MCMC package

You may be wondering why, well:

1.  Allows running multiple chains simultaneously (parallel)

2.  Overall faster than other Metrop MCMC algorithms (from our experience)

3.  Planning to include other types of kernels (the Handbook of MCMC)

4.  Implements reflective boundaries random-walk kernel

## Example: MCMC {.t}

```{r amcmc-data, echo=FALSE}
# Parameters
set.seed(1231)
n <- 1e3
pars <- c(mean = 2.6, sd = 3)

# Generating data and writing the log likelihood function
D <- rnorm(n, pars[1], pars[2])
```


```{r amcm-example, echo=TRUE, cache=TRUE}
# Loading the packages
library(amcmc)
library(coda)

# Defining the ll function (data was already defined)
ll <- function(x, D) {
  x <- log(dnorm(D, x[1], x[2]))
  sum(x)
}

ans <- MCMC(
  # Ll function and the starting parameters
  ll, c(mu=1, sigma=1),
  # How many steps, thinning, and burn-in
  nbatch = 1e5, thin=10, burnin = 1e4,
  # Kernel parameters
  scale = .1, ub = 10, lb = c(-10, 0),
  # How many parallel chains
  nchains = 4,
  # Further arguments passed to ll
  D=D
  )

```

## Example: MCMC (cont. 1) {.t}

```{r amcmc-gelmanplot, fig.cap='Gelman diagnostic for convergence. The closer to 1, the better the convergence. Rule of thumb: A chain has a reasonable convergence if it has a Potential Scale Reduction Factor (PSRF) below 1.15.', cache=TRUE}
coda::gelman.plot(ans)
```

<!-- is an unbiased estimator of the marginal posterior variance of θ (Gelman and Rubin 1992). The potential scale reduction factor (PSRF) is defined to be the ratio of Vˆ and W. If the M chains have converged to the target posterior distribution, then PSRF should be close to 1. Brooks and Gelman (1997) corrected the original PSRF by accounting for sampling variability as follows: -->
<!-- Rc=d̂ +3d̂ +1VˆW‾‾‾‾‾‾‾‾‾√ -->

<!-- where d̂  is the degrees of freedom estimate of a t distribution. -->

## Example: MCMC (cont. 2) {.t}

```{r amcmc-plot, fig.cap='Posterior distribution', cache=TRUE}
oldpar <- par(no.readonly = TRUE)
# par(mar = c(2.2,2.2,2,2), las=1, xpd=NA)
plot(ans)
par(oldpar)
```

# Bayesian Estimation of the parameters

## Agenda {.t}

\tableofcontents[currentsection]

## Putting all together 

Let's start by reading some data 

```{r read-panther, echo=TRUE}
# Reading the data
path <- system.file("tree.tree", package="aphylo")
dat <- read_panther(path)

# The tree
dat$tree
```

## Putting all together (cont.)

```{r printing-panther, echo=TRUE}
# Extra annotations
head(dat$internal_nodes_annotations)
```


## Putting all together: MCMC of the model {.t}

In this example, using data from PANTHERDB, we will simulate a single function and use the `aphylo_mcmc` function for obtaining parameter estimates

```{r sim-w-panther, echo=TRUE, cache=TRUE}
tree <- dat$tree

# Simulating a function
set.seed(123)
atree <- sim_annotated_tree(
  tree= as_po_tree(tree),
  Pi = .05, mu = c(.1, .05), psi = c(.01, .02)
)

# Estimation
ans <- aphylo_mcmc(
  params  = rep(.05, 5),
  dat     = atree,
  # Passing a Beta prior
  priors  = function(p) dbeta(p, 2, 20),
  # Parameters for the MCMC
  control = list(nchain=4, nbatch=1e4, thin=20, burnin=1e3)
  )
```



## Putting all together: MCMC of the model (cont. 1) {.t}

```{r panther-output, echo=TRUE}
ans
```

## How good is our prediction {.t}

```{r print-pred-score, echo=TRUE}
# Looking at the posterior probabilities
head(predict(ans, what="leafs"))

# And to the prediction score
prediction_score(ans)
```

## How good is our prediction (cont. 1) {.t}

```{r plot-pred-score, fig.cap="Predicted versus Observed values. Each slice of the pie represents a gene, the outer half of a slice is the predicted value, while the inner half is the observed value. Good predictions will coincide in color and show the slice closer to the center of the plot.", out.width=".6\\linewidth", echo=TRUE}
plot(prediction_score(ans), main="")
```

## A simulation study {.t label=sim-setup}

\framesubtitle{Setup}

-   Simulation study using ~13,000 families from PantherDB

-   Using a Beta 1/20 prior, we simulated annotations:
    
    *   Draw a set of the parameters $\{\psi_0,\psi_1 ,\mu_0, \mu_1,\pi\}$, 
    
    *   Simulated annotations using our model's Data Generating Process,
    
    *   Randomly removed $p\in [.1, .5]$ proportion of annotations.

-   With that data, we did parameter estimation and computed prediction scores using
    
    *   MLE
    *   MCMC with the right prior (Beta 1/20), and
    *   MCMC with the wrong prior (Beta 1/10, twice the mean as the right prior).

\hyperlink{sim-convergence}{\beamerbutton{Convergence}}

## A simulation study {.t}

\framesubtitle{Bias}

\begin{figure}
\centering
\includegraphics[width=.68\linewidth]{bias_trees_of_size_[57,138).pdf}
\end{figure}

## A simulation study {.t}

\framesubtitle{Prediction scores}

\begin{figure}
\centering
\includegraphics[width=.6\linewidth, trim = {0 1cm 0 2cm}, clip]{mcmc_right_prior_prediction.pdf}
\caption{Distribution of prediction scores. The random prediction scores were computed analytically with parameter $p=0.3$ (as resulting from the DGP).}
\end{figure}


# Concluding Remarks

## Concluding Remarks

-   A parsimonious model of gene functions: easy to apply on a large scale (we already ran some simulations using all 14,000 trees from the Panther DB).\pause

-   Already implemented, we are currently in the stage of writing the paper and setting up the simulation study.\pause


-   For the next steps, we are evaluating whether to include or how to include:\pause
    
    *   Type of node: speciation, duplication, horizontal transfer.
    
    *   Branch lengths
    
    *   Correlation structure between functions
    
    *   Using Taxon Constraints to improve predictions
    
    *   Hierarchical model: Use fully annotated trees by curators as prior information.

##

\begin{center}
\Huge
\color{USCCardinal}{\textbf{Thank you!}}
\end{center}

\maketitle


\appendix

## Formal definitions {.t label=formaldef}

\framesubtitle{\hyperlink{definitions}{\beamerbutton{go back}}}

1.  Phylogenetic tree: In our case, we talk about \textcolor{red}{partially ordered} phylogenetic tree, in particular, $\phylo\equiv (N,E)$ is a tuple of nodes $N$, and edges

    $$
    E\equiv \{(n, m) \in N\times N: n\mapsto m, \textcolor{red}{n < m}\}
    $$

2.  Offspring of $n$: $O(n)\equiv\{m\in N: (n, m) \in E, n\in N\}$

3.  Parent node of $m$: $r(m) \equiv\{n \in N: (n, m) \in E, m\in N\}$

4.  Leaf nodes: $\Leaf(\phylo)\equiv \{m \in N: O(m)=\{\emptyset\}\}$

5.  Annotations: Given $P$ functions, $\Ann \equiv \{\ann_n \in \{0,1\}^P: n\in \Leaf(\phylo)\}$

6.  Annotated Phylogenetic Tree $\aphylo \equiv(\phylo, \Ann)$

7.  Observed Annotated Annotations $\AnnObs = \{\annObs_l\}_{l\in \Leaf(\phylo)}$,

8.  Experimentally Annotated Phylogenetic Tree $\aphyloObs\equiv(\phylo, \AnnObs)$


## Leaf node probabilities {.t label=leafnodesprob}

\framesubtitle{\hyperlink{peelingalgorithm}{\beamerbutton{go back}}}

-   The probability of the leaf nodes having annotations $\ann_l$ conditional on the observed annotation is 
    
    \begin{equation}
    \label{eq:leaf1}
    \Prcond{\Ann_{l} = \ann_{l}}{\AnnObs_{l} = \annObs_{l}} = \left\{
    \begin{array}{ll}
    \psi &\mbox{if }\annObs_{l} \neq \ann_{l} \\
    1 - \psi & \mbox{otherwise}
    \end{array}
    \right.
    \end{equation}
    
    Where $\psi$ can be either $\psi_0$ (mislabelling a zero), or $\psi_1$ (mislabelling a one).
    

## Internal node probabilities {.t label=internalnodeprob}

\framesubtitle{\hyperlink{peelingalgorithm}{\beamerbutton{go back}}}

-   In the case of the internal nodes, the probability of a given state is defined in terms of the gain/loss probabilities
    
    $$
    \Prcond{\Ann_{n} = \ann_{lp}}{\Ann_{r(n)} = \ann_{r(n)}} = \left\{
    \begin{array}{ll}
    \mu & \mbox{if }\ann_{n} \neq \ann_{r(n)} \\
    1 - \mu & \mbox{otherwise}
    \end{array}
    \right.
    $$
    
    Where $\mu$ can be either $\mu_0$ (gain), or $\mu_1$ (loss).
    
-   Assuming independence accross offspring, we can write
    
    
    \begin{multline}
    \label{eq:interior1}
    \Prcond{\Ann_n = \ann_n}{\aphyloObs_n} = %
    \prod_{m \in O(n)} \sum_{\ann_m \in \{0,1\}} \Prcond{\Ann_m = \ann_m}{\aphyloObs_m} \\
    \Prcond{\Ann_{m} = \ann_{m}}{\Ann_{n} = \ann_{n}}
    \end{multline}
    
    Notice that if $m$ is a leaf node, then $\Prcond{\Ann_m = \ann_m}{\aphyloObs_m} = \Prcond{\Ann_m = \ann_m}{\AnnObs_m = \annObs_m}$.

## Likelihood of the tree {.t label=likelihood}

\framesubtitle{\hyperlink{peelingalgorithm}{\beamerbutton{go back}}}

-   Once the computation reaches the root node, $n=0$, equations \eqref{eq:leaf1} and \eqref{eq:interior1}: 
    
    \tiny
    
    \begin{align*}
    \Prcond{\Ann_l = \ann_l}{\aphyloObs_l} & = \Prcond{\Ann_{l} = \ann_{lp}}{\AnnObs_{l} = \annObs_{l}} \tag{\ref{eq:leaf1}}\\
    \Prcond{\Ann_n = \ann_n}{\aphyloObs_n}  & = 
    \prod_{m \in O(n)} \sum_{\ann_m \in \{0,1\}} \Prcond{\Ann_m = \ann_m}{\aphyloObs_m}  
    \Prcond{\Ann_{m} = \ann_{m}}{\Ann_{n} = \ann_{n}} \tag{\ref{eq:interior1}}
    \end{align*}
    
    
    \normalsize
    
    Allow us writing the likelihood of the entire tree
    
    \begin{equation}
    \label{eq:l}
    \likelihood{\psi, \mu, \pi}{\aphyloObs} = \sum_{\ann_0\in\{0,1\}}\Prcond{\Ann_0=\ann_0}{\pi}\Prcond{\Ann_0=\ann_0}{\aphyloObs_0}
    \end{equation}
    
    Where $\Prcond{\Ann_0=\ann_0}{\pi} = \pi^{\ann_{0}}\left(1 - \pi\right)^{1 - \ann_{0}}$

## A simulation study {.t label=sim-convergence}

\framesubtitle{Convergence \hyperlink{sim-setup}{\beamerbutton{go back}}}

\begin{figure}
\centering
\includegraphics[width=.6\linewidth, trim={0 1.5cm 0 2cm},clip]{gelmans_right_prior.pdf}
\caption{Gelman diagnostic for convergence. The closer to 1, the better the convergence. Rule of thumb: A chain has a reasonable convergence if it has a Potential Scale Reduction Factor (PSRF) below 1.15.}
\end{figure}
